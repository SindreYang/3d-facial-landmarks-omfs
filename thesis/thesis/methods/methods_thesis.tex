\documentclass[class=article, crop=false]{standalone}


\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{import}
\usepackage{multicol}
\usepackage[subpreambles=false]{standalone}
\usepackage{tikz}

\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\newcommand\commentfont[1]{\footnotesize\ttfamily\textcolor{blue}{#1}}
\SetCommentSty{commentfont}

\usepackage[backend=biber]{biblatex}
\addbibresource{references.bib}

\usepackage{geometry}
\geometry{
   a4paper,
   left=20mm, right=20mm,
   top=20mm, bottom=25mm
}
 
\usepackage{hyperref}
\hypersetup{
  colorlinks=true,
  linkcolor=blue,
  citecolor=black,
}


\begin{document}




\section{Methods}
\label{sec:methods}

\subsection{Data set}
\import{}{import/stereophoto.tex}
The models are trained predominantly on the Headspace dataset \cite{Dai2019}, a set of 3D images of the human head that is available for university-based non-commercial research. The collection consists of 1519 subjects each wearing tight fitting latex caps. This is done to avoid holes in the mesh on the scalp of the patient. The photos are captured by a 5-camera setup around the head of the person (Fig. \ref{fig:stereophotogrammetry}). The images have a high quality, consistent illumination, and are pose normalized. 1200 samples include annotations, but as they were automatically generated, the quality of the labels differs for each landmark type and for each sample. Nonetheless, due to the sheer number of annotated samples, the Headspace data is used as training data for the first network that extracts rough landmark locations. Manual inspection shows that the Zhu-Ramanan mixture of trees algorithm in combination with the subsequent 2D to 3D projection make repeating errors such as consistently placing the landmarks a little too low. %'although often inaccurate predictions, they are consistent (note: move to discussion). error patterns 
In total, the headspace data set comes with 68 landmarks per image. However, many of them are non-surgical and ill-defined and can be discarded for the purpose of finding landmarks to assist oral and maxillofacial surgery. Out of the 68 landmarks we keep 12 medically relevant, anatomical landmarks (see table \ref{table:landmark_names}). For properly evaluating the results and for training the refinement network, we manually annotate those 12 landmark positions for around 350 Headspace samples. Although not placed by a specialist, these labels are considerably more accurate, as they are human-annotated as opposed to machine-annotated.
Furthermore, we use 3D photos from Radboudumc to further validate our method. The in-house meshes have a lower quality and the illumination varies greatly. The 3D photos are not pose normalized and differ and differ substantially to the head positions in the Headspace data set. As the patients do not wear any latex-caps, holes and artefacts in the region of the hair is common. Also, most 3D photos are captured by a 2-camera-setup, namely one from the front-left and one from the front-right. This means that meshes reconstructed from such photos have big holes in the back of the cranium as they only capture the frontal view.
% Furthermore, we use  % Radboudumc data. As the reconstruction of the mesh for subjects with long hair and hair styles that do not form a flat surface is especially difficult

\subsection{Pre-processing}
The 3D meshes are stored as ‘wavefront object’ files (.obj file). This  file format contains information for vertices, edges, faces, normal vectors and texture.
Vertices are points in the Cartesian coordinate system defined by x, y and z. Meshes also contain surface data in the form of edges and faces that define the interconnectivity between vertices. Normals \cite{Jong}.
To simplify the problem, our network processes point clouds instead of meshes.
The meshes of the Headspace dataset are already pose normalized.

The 68 landmarks in the Headspace data are given by a reference to the vertex index in the mesh. The manually annotated landmarks in 3DMedX\footnote{3DMedX is a software from Radboudumc that allows 3D reconstruction using DICOM files from (CB)CT-scans or MRI scans and offers tools for the evaluation of orthognathic surgery. It also supports the creation of custom workflows e.g. for registering landmarks.}\cite{3dmedx} are saved as coordinates in Comma-separated values files. As the meshes are simplified before being fed into the network,%explain simplification
the original landmark coordinates do not point to a vertex in the downsampled mesh anymore and have to be re-calculated. The corresponding landmark point for the downsampled mesh is re-calculated by picking the vertex with the smallest distance to the original coordinate. Currently, this is done in a naive way by iterating over each vertex in the mesh. This step can be sped up significantly by applying a more sophisticated algorithm. The ground truth not only consists of point landmarks as it is difficult to train neural networks on such sparse positive cases. Instead, we create point clusters (heatmaps) around the landmark point to create regions that the network can learn more easily. The point closest to the landmark has the highest activation (1.0), points in the 3mm neighborhood are assigned an activation of 0.75, in the 4.5mm neighborhood 0.5 and in the 6mm neighborhood 0.25. An alternative, perhaps more balanced solution would be to create a gaussian heatmap. The heatmap approach increases the proportion of points with an activation higher than zero and improves the class imbalance problem.

To build a landmark detector that can discriminate between landmark types and to allow for overlapping activation clusters (meaning one point can be part of the neighborhood of multiple landmarks), each landmark cluster is stored in its own channel..

However, storing the activation for each point is very memory-consuming. To compress the label files, only the vertex indices and activation for points with an activation higher than zero are explicitly stored. All other points are assumed to have an activation equal to zero. This reduces the necessary information to store from $\textit{total vertices} \times \textit{channels}$ for the sparse matrix representation to $\textit{verices in landmark regions} \times \textit{2} \times \textit{channels}$ where the factor 2 arises as the compressed representation not only needs to save the activation but also additional vertex index information.
%Labels sparse representation why? However, this results in 

\subsection{DiffusionNet}
We define a point set \begin{math}X=\{X_i \in \mathbb{R}^F,\hspace{0.3cm}i = 1,2,...,N\}\end{math} as the input of our model, where N defines the number of points in the point cloud, F the dimension, and $x_i$ is the 3D coordinate of each point in the Cartesian reference system. Note, that even in a 3-dimensional reference system, $F$ is not restricted to 3 as we can use other point-based features such as the color or the normal vector with respect to the surface.

Graph-based method in spatial-domain combined with a point-wise MLP. Spectral methods are used for accelerating for the computation of the diffusion operation.
Most machine learning algorithms require a fixed input size. DiffusionNet is able to deal with a flexible input size, making sampling or simplification for the purpose of standardizing the number of vertices unnecessary.

\subsection{Shape Variants}
3D shapes can come in different variants that the network should be invariant to, such as different orientations or different discretization. Different camera setups or different pre-processing can lead to very different orientations of the head in the space. The network should give the same result regardless of how the head is rotated. The perhaps most straightforward approach is to perform data augmentation. While it can work to make the network more robust to the presented augmented variants, data augmentation does not scale well as it is not feasible to sample all variations. Additionally, including slightly varied samples in the training quickly increases training times. The preferred approach to deal with shape variants is to design a network that is inherently invariant to rotations. There is still ongoing research on how to most efficiently design such invariant networks. One way is to use input features such as the Heat Kernel Signature (HKS) that are invariant to isometric deformations, thus also to different poses. DiffusionNet deals with the problem by... adds robustness but not true invariance.



\end{document}
