
@article{Plooij2009,
   abstract = {In 3D photographs the bony structures are neither available nor palpable, therefore, the bone-related landmarks, such as the soft tissue gonion, need to be redefined. The purpose of this study was to determine the reproducibility and reliability of 49 soft tissue landmarks, including newly defined 3D bone-related soft tissue landmarks with the use of 3D stereophotogrammetric images. Two observers carried out soft-tissue analysis on 3D photographs twice for 20 patients. A reference frame and 49 landmarks were identified on each 3D photograph. Paired Student's t-test was used to test the reproducibility and Pearson's correlation coefficient to determine the reliability of the landmark identification. Intra- and interobserver reproducibility of the landmarks were high. The study showed a high reliability coefficient for intraobserver (0.97 (0.90 - 0.99)) and interobserver reliability (0.94 (0.69 - 0.99)). Identification of the landmarks in the midline was more precise than identification of the paired landmarks. In conclusion, the redefinition of bone-related soft tissue 3D landmarks in combination with the 3D photograph reference system resulted in an accurate and reliable 3D photograph based soft tissue analysis. This shows that hard tissue data are not needed to perform accurate soft tissue analysis. © 2008 International Association of Oral and Maxillofacial Surgeons.},
   author = {J. M. Plooij and G. R.J. Swennen and F. A. Rangel and T. J.J. Maal and F. A.C. Schutyser and E. M. Bronkhorst and A. M. Kuijpers-Jagtman and S. J. Bergé},
   doi = {10.1016/j.ijom.2008.12.009},
   issn = {09015027},
   issue = {3},
   journal = {International Journal of Oral and Maxillofacial Surgery},
   title = {Evaluation of reproducibility and reliability of 3D soft tissue analysis using 3D stereophotogrammetry},
   volume = {38},
   year = {2009},
}


@article{Ras1996,
   abstract = {Objectives: The purpose of this study was to introduce stereophotogrammetry as a three-dimensional registration method for quantifying facial morphology and detecting changes in facial morphology during growth and development. Methods: Using stereophotogrammetry, three-dimensional (3-D) co-ordinates for the bilateral landmarks Exocanthion and Cheilion and the midsagittal landmark Pronasale were determined in 10 subjects to ascertain the reproducibility of the method, and in 59 children to detect changes in facial morphology due to growth and development. Linear and angular measurements were calculated by means of the 3-D co-ordinates in order to quantify facial morphology. Significant differences were determined by means of analyses of variance (MANOVA). Results: During the observation period, significant (P < 0.01) changes in facial morphology were determined for the linear measurements. Advantages and disadvantages of current registration methods are discussed. Conclusion: It is concluded that stereophotogrammetry is a suitable 3-D registration method for quantifying and detecting developmental changes in facial morphology. Copyright © 1996 Elsevier Science Ltd.},
   author = {F. Ras and L. L.M.H. Habets and F. C. Van Ginkel and B. Prahl-Andersen},
   doi = {10.1016/0300-5712(95)00081-X},
   issn = {03005712},
   issue = {5},
   journal = {Journal of Dentistry},
   title = {Quantification of facial morphology using stereophotogrammetry - Demonstration of a new concept},
   volume = {24},
   year = {1996},
}
@article{Bidra2009,
   abstract = {Statement of problem: The importance of the midline is well known to dentists. Currently, there are no verifiable guidelines that direct the choice of specific anatomic landmarks to determine the midline of the face or midline of the mouth. Purpose: The purpose of this study was to determine the hierarchy of facial anatomic landmarks closest to the midline of the face as well as midline of the mouth. Material and methods: Three commonly used anatomic landmarks, nasion, tip of the nose, and tip of the philtrum, were marked clinically on 249 subjects (age range: 21-45 years). Frontal full-face digital images of the subjects in smile were then made under standardized conditions. A total of 107 subjects met the inclusion criteria. Upon applying exclusion criteria, images of 87 subjects were used for midline analysis using a novel concept called the Esthetic Frame. Deviations from the midlines of the face and mouth were measured for the 3 clinical landmarks; the existing dental midline was considered as the fourth landmark. The entire process of midline analysis was done by a single observer and repeated twice. Reliability analysis and 1-sample t tests were conducted at alpha values of .001 and .05, respectively. Results: The results indicated that each of the 4 landmarks deviated uniquely and significantly (P<.001) from the midlines of the face as well as the mouth. Conclusions: Within the limitations of the study, the hierarchy of anatomic landmarks closest to the midline of the face in smile was as follows: the midline of the oral commissures, natural dental midline, tip of philtrum, nasion, and tip of the nose. The hierarchy of anatomic landmarks closest to the midline of the oral commissures was: natural dental midline, tip of philtrum, tip of the nose, and nasion. These relationships were the same for both genders and all ethnicities classified. © 2009 The Editorial Council of the Journal of Prosthetic Dentistry.},
   author = {Avinash S. Bidra and Flavio Uribe and Thomas D. Taylor and John R. Agar and Patchnee Rungruanganunt and William P. Neace},
   doi = {10.1016/S0022-3913(09)60117-7},
   issn = {00223913},
   issue = {2},
   journal = {Journal of Prosthetic Dentistry},
   title = {The relationship of facial anatomic landmarks with midlines of the face and mouth},
   volume = {102},
   year = {2009},
}

@article{Dai2019,
author="Dai, Hang and Pears, Nick and Smith, William and Duncan, Christian",
title="Statistical Modeling of Craniofacial Shape and Texture",
journal="International Journal of Computer Vision",
year="2019",
month="Nov",
day="09",
volume = "128",
number="2",
pages="547-571",
abstract="We present a fully-automatic statistical 3D shape modeling approach and apply it to a large dataset of 3D images, the Headspace dataset, thus generating the first public shape-and-texture 3D morphable model (3DMM) of the full human head. Our approach is the first to employ a template that adapts to the dataset subject before dense morphing. This is fully automatic and achieved using 2D facial landmarking, projection to 3D shape, and mesh editing. In dense template morphing, we improve on the well-known Coherent Point Drift algorithm, by incorporating iterative data-sampling and alignment. Our evaluations demonstrate that our method has better performance in correspondence accuracy and modeling ability when compared with other competing algorithms. We propose a texture map refinement scheme to build high quality texture maps and texture model. We present several applications that include the first clinical use of craniofacial 3DMMs in the assessment of different types of surgical intervention applied to a craniosynostosis patient group.",
issn="1573-1405",
doi="10.1007/s11263-019-01260-7",
url="https://doi.org/10.1007/s11263-019-01260-7"
}

@generic{Guo2021,
   abstract = {Point cloud learning has lately attracted increasing attention due to its wide applications in many areas, such as computer vision, autonomous driving, and robotics. As a dominating technique in AI, deep learning has been successfully used to solve various 2D vision problems. However, deep learning on point clouds is still in its infancy due to the unique challenges faced by the processing of point clouds with deep neural networks. Recently, deep learning on point clouds has become even thriving, with numerous methods being proposed to address different problems in this area. To stimulate future research, this paper presents a comprehensive review of recent progress in deep learning methods for point clouds. It covers three major tasks, including 3D shape classification, 3D object detection and tracking, and 3D point cloud segmentation. It also presents comparative results on several publicly available datasets, together with insightful observations and inspiring future research directions.},
   author = {Yulan Guo and Hanyun Wang and Qingyong Hu and Hao Liu and Li Liu and Mohammed Bennamoun},
   doi = {10.1109/TPAMI.2020.3005434},
   issn = {19393539},
   issue = {12},
   journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
   title = {Deep Learning for 3D Point Clouds: A Survey},
   volume = {43},
   year = {2021},
}

@inproceedings{Zhu2012,
   abstract = {We present a unified model for face detection, pose estimation, and landmark estimation in real-world, cluttered images. Our model is based on a mixtures of trees with a shared pool of parts; we model every facial landmark as a part and use global mixtures to capture topological changes due to viewpoint. We show that tree-structured models are surprisingly effective at capturing global elastic deformation, while being easy to optimize unlike dense graph structures. We present extensive results on standard face benchmarks, as well as a new in the wild annotated dataset, that suggests our system advances the state-of-the-art, sometimes considerably, for all three tasks. Though our model is modestly trained with hundreds of faces, it compares favorably to commercial systems trained with billions of examples (such as Google Picasa and face.com). © 2012 IEEE.},
   author = {Xiangxin Zhu and Deva Ramanan},
   doi = {10.1109/CVPR.2012.6248014},
   issn = {10636919},
   journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
   title = {Face detection, pose estimation, and landmark localization in the wild},
   year = {2012},
}

@misc{Jong,
  author        = {Guido de Jong},
  title         = {Lecture notes: 3D Mesh Processing and Analysis, 3D Computer vision for Medical Applications},
  month         = {October},
  year          = {2021},
}

@inproceedings{yan:hal-02892002,
  TITLE = {{A survey of deep facial landmark detection}},
  AUTHOR = {Yan, Yongzhe and Naturel, Xavier and Chateau, Thierry and Duffner, Stefan and Garcia, Christophe and Blanc, Christophe},
  URL = {https://hal.archives-ouvertes.fr/hal-02892002},
  BOOKTITLE = {{RFIAP}},
  ADDRESS = {Paris, France},
  YEAR = {2018},
  MONTH = Jun,
  KEYWORDS = {Facial landmark detection ; Face alignment ; Deep learning ; Deep learning ; D{\'e}tection de landmark facial ; Alignement de visage},
  PDF = {https://hal.archives-ouvertes.fr/hal-02892002/file/RFIAP_2018_Yan_A.pdf},
  HAL_ID = {hal-02892002},
  HAL_VERSION = {v1},
}

@article{perrot:hal-02884592,
  TITLE = {{Implementing Cascade of Regression-based Face Landmarking: an in-Depth Overview}},
  AUTHOR = {Perrot, Romuald and Bourdon, Pascal and Helbert, David},
  URL = {https://hal.archives-ouvertes.fr/hal-02884592},
  JOURNAL = {{Image and Vision Computing}},
  PUBLISHER = {{Elsevier}},
  VOLUME = {102},
  PAGES = {103976},
  YEAR = {2020},
  MONTH = Oct,
  DOI = {10.1016/j.imavis.2020.103976},
  KEYWORDS = {Face landmarking ; Regression trees ; Temporal tracking},
  PDF = {https://hal.archives-ouvertes.fr/hal-02884592/file/main-IVC-hal.pdf},
  HAL_ID = {hal-02884592},
  HAL_VERSION = {v1},
}

@inproceedings{aflw,
   abstract = {Face alignment is a crucial step in face recognition tasks. Especially, using landmark localization for geometric face normalization has shown to be very effective, clearly improving the recognition results. However, no adequate databases exist that provide a sufficient number of annotated facial landmarks. The databases are either limited to frontal views, provide only a small number of annotated images or have been acquired under controlled conditions. Hence, we introduce a novel database overcoming these limitations: Annotated Facial Landmarks in the Wild (AFLW). AFLW provides a large-scale collection of images gathered from Flickr, exhibiting a large variety in face appearance (e.g., pose, expression, ethnicity, age, gender) as well as general imaging and environmental conditions. In total 25,993 faces in 21,997 real-world images are annotated with up to 21 landmarks per image. Due to the comprehensive set of annotations AFLW is well suited to train and test algorithms for multi-view face detection, facial landmark localization and face pose estimation. Further, we offer a rich set of tools that ease the integration of other face databases and associated annotations into our joint framework. © 2011 IEEE.},
   author = {Martin Köstinger and Paul Wohlhart and Peter M. Roth and Horst Bischof},
   doi = {10.1109/ICCVW.2011.6130513},
   journal = {Proceedings of the IEEE International Conference on Computer Vision},
   title = {Annotated facial landmarks in the wild: A large-scale, real-world database for facial landmark localization},
   year = {2011},
}

@article{sharp2022diffusion,
  author = {Sharp, Nicholas and Attaiki, Souhaib and Crane, Keenan and Ovsjanikov, Maks},
  title = {DiffusionNet: Discretization Agnostic Learning on Surfaces},
  journal = {ACM Trans. Graph.},
  volume = {01},
  number = {1},
  year = {2022},
  publisher = {ACM},
  address = {New York, NY, USA},
}

@inproceedings{Qi2017,
abstract = {Point cloud is an important type of geometric data structure. Due to its irregular format, most researchers transform such data to regular 3D voxel grids or collections of images. This, however, renders data unnecessarily voluminous and causes issues. In this paper, we design a novel type of neural network that directly consumes point clouds, which well respects the permutation invariance of points in the input. Our network, named PointNet, provides a unified architecture for applications ranging from object classification, part segmentation, to scene semantic parsing. Though simple, PointNet is highly efficient and effective. Empirically, it shows strong performance on par or even better than state of the art. Theoretically, we provide analysis towards understanding of what the network has learnt and why the network is robust with respect to input perturbation and corruption.},
archivePrefix = {arXiv},
arxivId = {1612.00593},
author = {Qi, Charles R. and Su, Hao and Mo, Kaichun and Guibas, Leonidas J.},
booktitle = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
doi = {10.1109/CVPR.2017.16},
eprint = {1612.00593},
isbn = {9781538604571},
title = {{PointNet: Deep learning on point sets for 3D classification and segmentation}},
year = {2017}
}

@inproceedings{Qi2017b,
abstract = {Few prior works study deep learning on point sets. PointNet [20] is a pioneer in this direction. However, by design PointNet does not capture local structures induced by the metric space points live in, limiting its ability to recognize fine-grained patterns and generalizability to complex scenes. In this work, we introduce a hierarchical neural network that applies PointNet recursively on a nested partitioning of the input point set. By exploiting metric space distances, our network is able to learn local features with increasing contextual scales. With further observation that point sets are usually sampled with varying densities, which results in greatly decreased performance for networks trained on uniform densities, we propose novel set learning layers to adaptively combine features from multiple scales. Experiments show that our network called PointNet++ is able to learn deep point set features efficiently and robustly. In particular, results significantly better than state-of-the-art have been obtained on challenging benchmarks of 3D point clouds.},
archivePrefix = {arXiv},
arxivId = {1706.02413},
author = {Qi, Charles R. and Yi, Li and Su, Hao and Guibas, Leonidas J.},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1706.02413},
issn = {10495258},
title = {{PointNet++: Deep hierarchical feature learning on point sets in a metric space}},
year = {2017}
}

@inproceedings{Li2018,
abstract = {We present a simple and general framework for feature learning from point clouds. The key to the success of CNNs is the convolution operator that is capable of leveraging spatially-local correlation in data represented densely in grids (e.g. images). However, point clouds are irregular and unordered, thus directly convolving kernels against features associated with the points will result in desertion of shape information and variance to point ordering. To address these problems, we propose to learn an X-transformation from the input points to simultaneously promote two causes: the first is the weighting of the input features associated with the points, and the second is the permutation of the points into a latent and potentially canonical order. Element-wise product and sum operations of the typical convolution operator are subsequently applied on the X-transformed features. The proposed method is a generalization of typical CNNs to feature learning from point clouds, thus we call it PointCNN. Experiments show that PointCNN achieves on par or better performance than state-of-the-art methods on multiple challenging benchmark datasets and tasks.},
author = {Li, Yangyan and Bu, Rui and Sun, Mingchao and Wu, Wei and Di, Xinhan and Chen, Baoquan},
booktitle = {Advances in Neural Information Processing Systems},
issn = {10495258},
title = {{PointCNN: Convolution on X-transformed points}},
year = {2018}
}

@article{Hanocka2019,
abstract = {Polygonal meshes provide an efficient representation for 3D shapes. They explicitly capture both shape surface and topology, and leverage non-uniformity to represent large flat regions as well as sharp, intricate features. This non-uniformity and irregularity, however, inhibits mesh analysis efforts using neural networks that combine convolution and pooling operations. In this paper, we utilize the unique properties of the mesh for a direct analysis of 3D shapes using MeshCNN, a convolutional neural network designed specifically for triangular meshes. Analogous to classic CNNs, MeshCNN combines specialized convolution and pooling layers that operate on the mesh edges, by leveraging their intrinsic geodesic connections. Convolutions are applied on edges and the four edges of their incident triangles, and pooling is applied via an edge collapse operation that retains surface topology, thereby, generating new mesh connectivity for the subsequent convolutions. MeshCNN learns which edges to collapse, thus forming a task-driven process where the network exposes and expands the important features while discarding the redundant ones. We demonstrate the effectiveness of MeshCNN on various learning tasks applied to 3D meshes.},
author = {Hanocka, Rana and Hertz, Amir and Fish, Noa and Giryes, Raja and Fleishman, Shachar and Cohen-Or, Daniel},
doi = {10.1145/3306346.3322959},
issn = {0730-0301},
journal = {ACM Transactions on Graphics},
title = {{MeshCNN}},
year = {2019}
}

@misc{Bello2020,
abstract = {A point cloud is a set of points defined in a 3D metric space. Point clouds have become one of the most significant data formats for 3D representation and are gaining increased popularity as a result of the increased availability of acquisition devices, as well as seeing increased application in areas such as robotics, autonomous driving, and augmented and virtual reality. Deep learning is now the most powerful tool for data processing in computer vision and is becoming the most preferred technique for tasks such as classification, segmentation, and detection. While deep learning techniques are mainly applied to data with a structured grid, the point cloud, on the other hand, is unstructured. The unstructuredness of point clouds makes the use of deep learning for its direct processing very challenging. This paper contains a review of the recent state-of-the-art deep learning techniques, mainly focusing on raw point cloud data. The initial work on deep learning directly with raw point cloud data did not model local regions; therefore, subsequent approaches model local regions through sampling and grouping. More recently, several approaches have been proposed that not only model the local regions but also explore the correlation between points in the local regions. From the survey, we conclude that approaches that model local regions and take into account the correlation between points in the local regions perform better. Contrary to existing reviews, this paper provides a general structure for learning with raw point clouds, and various methods were compared based on the general structure. This work also introduces the popular 3D point cloud benchmark datasets and discusses the application of deep learning in popular 3D vision tasks, including classification, segmentation, and detection.},
archivePrefix = {arXiv},
arxivId = {2001.06280},
author = {Bello, Saifullahi Aminu and Yu, Shangshu and Wang, Cheng and Adam, Jibril Muhmmad and Li, Jonathan},
booktitle = {Remote Sensing},
doi = {10.3390/rs12111729},
eprint = {2001.06280},
issn = {20724292},
keywords = {Classification,Datasets,Deep learning,Object detection,Point cloud,Segmentation},
title = {{Review: Deep learning on 3D point clouds}},
year = {2020}
}

% @misc{3dmedx,
%   title = {{3DMedX® (v1.2.23.0), 3D Lab Radboudumc, Nijmegen: The all-in-one solution for 3D research and the home of the OrthoGnathicAnalyser.},
%   howpublished = {\url{https://www.3dmedx.nl}},
%   note = {Accessed: 2022-02-04}
% }

@online{3dmedx,
  author = {3D Lab Radboudumc Nijmegen},
  title = {{3DMedX® (v1.2.23.0)}, The all-in-one solution for 3D research and the home of the OrthoGnathicAnalyser.},
  url = {https://www.3dmedx.nl},
  urldate = {2022-02-04}
}

@article{Cao2020,
   abstract = {Deep learning methods have achieved great success in analyzing traditional data such as texts, sounds, images and videos. More and more research works are carrying out to extend standard deep learning technologies to geometric data such as point cloud or voxel grid of 3D objects, real life networks such as social and citation network. Many methods have been proposed in the research area. In this work, we aim to provide a comprehensive survey of geometric deep learning and related methods. First, we introduce the relevant knowledge and history of geometric deep learning field as well as the theoretical background. In the method part, we review different graph network models for graphs and manifold data. Besides, practical applications of these methods, datasets currently available in different research area and the problems and challenges are also summarized.},
   author = {Wenming Cao and Zhiyue Yan and Zhiquan He and Zhihai He},
   doi = {10.1109/ACCESS.2020.2975067},
   issn = {21693536},
   journal = {IEEE Access},
   title = {A Comprehensive Survey on Geometric Deep Learning},
   volume = {8},
   year = {2020},
}

@online{sdsu,
  author = {San Diego State University},
  title = {{}, Visualizing Strain},
  url = {http://www.sci.sdsu.edu/visualstructure/vss/htm_hlp/def_d.htm},
  urldate = {2022-02-04}
}

@inproceedings{Sun2009,
   abstract = {We propose a novel point signature based on the properties of the heat diffusion process on a shape. Our signature, called the Heat Kernel Signature (or HKS), is obtained by restricting the well-known heat kernel to the temporal domain. Remarkably we show that under certain mild assumptions, HKS captures all of the information contained in the heat kernel, and characterizes the shape up to isometry. This means that the restriction to the temporal domain, on the one hand, makes HKS much more concise and easily commensurable, while on the other hand, it preserves all of the information about the intrinsic geometry of the shape. In addition, HKS inherits many useful properties from the heat kernel, which means, in particular, that it is stable under perturbations of the shape. Our signature also provides a natural and efficiently computable multi-scale way to capture information about neighborhoods of a given point, which can be extremely useful in many applications. To demonstrate the practical relevance of our signature, we present several methods for non-rigid multi-scale matching based on the HKS and use it to detect repeated structure within the same shape and across a collection of shapes. © 2009 The Author(s) Journal compilation © 2009 The Eurographics Association and Blackwell Publishing Ltd.},
   author = {Jian Sun and Maks Ovsjanikov and Leonidas Guibas},
   issn = {17278384},
   issue = {5},
   journal = {Eurographics Symposium on Geometry Processing},
   title = {A concise and provably informative multi-scale signature based on heat diffusion},
   volume = {28},
   year = {2009},
}

@online{stanfod_iso,
  author = {Justin Solomon, Stanford University},
  title = {{CS 468 Lecture 16:}, Isometry Invariance and Spectral Techniques},
  url = {https://graphics.stanford.edu/courses/cs468-13-spring/assets/lecture16-gawlik.pdf},
  urldate = {2022-02-04}
}

@INPROCEEDINGS{mvcnn,
  author={Su, Hang and Maji, Subhransu and Kalogerakis, Evangelos and Learned-Miller, Erik},
  booktitle={2015 IEEE International Conference on Computer Vision (ICCV)}, 
  title={Multi-view Convolutional Neural Networks for 3D Shape Recognition}, 
  year={2015},
  volume={},
  number={},
  pages={945-953},
  doi={10.1109/ICCV.2015.114}}
  
  @book{Botsch2010,
   abstract = {Geometry processing, or mesh processing, is a fast-growing area of research that uses concepts from applied mathematics, computer science, and engineering to design efficient algorithms for the acquisition, reconstruction, analysis, manipulation, simulation, and transmission of complex 3D models. Applications of geometry processing algorithms already cover a wide range of areas from multimedia, entertainment, and classical computer-aided design, to biomedical computing, reverse engineering, and scientific computing. Over the last several years, triangle meshes have become increasingly popular, as irregular triangle meshes have developed into a valuable alternative to traditional spline surfaces. This book discusses the whole geometry processing pipeline based on triangle meshes. The pipeline starts with data input, for example, a model acquired by 3D scanning techniques. This data can then go through processes of error removal, mesh creation, smoothing, conversion, morphing, and more. The authors detail techniques for those processes using triangle meshes. A supplemental website contains downloads and additional information.},
   author = {Mario Botsch and Leif Kobbelt and Mark Pauly and Pierre Alliez and Bruno Levy},
   doi = {10.1201/b10688},
   journal = {Polygon Mesh Processing},
   title = {Polygon Mesh Processing},
   year = {2010},
}

@Manual{blender, 
   title = {Blender - a 3D modelling and rendering package}, 
   author = {Blender Online Community}, 
   organization = {Blender Foundation}, 
   address = {Stichting Blender Foundation, Amsterdam}, 
   year = {2018}, 
   url = {http://www.blender.org}, 
 }
 
 @inproceedings {meshlab, 
 booktitle = {Eurographics Italian Chapter Conference}, editor = {Vittorio Scarano and Rosario De Chiara and Ugo Erra}, title = {{MeshLab: an Open-Source Mesh Processing Tool}}, author = {Cignoni, Paolo and Callieri, Marco and Corsini, Massimiliano and Dellepiane, Matteo and Ganovelli, Fabio and Ranzuglia, Guido}, year = {2008}, publisher = {The Eurographics Association}, ISBN = {978-3-905673-68-5}, DOI = {10.2312/LocalChapterEvents/ItalChap/ItalianChapConf2008/129-136} }
 
 @article{DBLP:journals/corr/abs-1904-08755,
  author    = {Christopher B. Choy and
               JunYoung Gwak and
               Silvio Savarese},
  title     = {4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1904.08755},
  year      = {2019},
  url       = {http://arxiv.org/abs/1904.08755},
  eprinttype = {arXiv},
  eprint    = {1904.08755},
  timestamp = {Mon, 24 Feb 2020 15:00:26 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1904-08755.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@generic{Guo2021,
   abstract = {Point cloud learning has lately attracted increasing attention due to its wide applications in many areas, such as computer vision, autonomous driving, and robotics. As a dominating technique in AI, deep learning has been successfully used to solve various 2D vision problems. However, deep learning on point clouds is still in its infancy due to the unique challenges faced by the processing of point clouds with deep neural networks. Recently, deep learning on point clouds has become even thriving, with numerous methods being proposed to address different problems in this area. To stimulate future research, this paper presents a comprehensive review of recent progress in deep learning methods for point clouds. It covers three major tasks, including 3D shape classification, 3D object detection and tracking, and 3D point cloud segmentation. It also presents comparative results on several publicly available datasets, together with insightful observations and inspiring future research directions.},
   author = {Yulan Guo and Hanyun Wang and Qingyong Hu and Hao Liu and Li Liu and Mohammed Bennamoun},
   doi = {10.1109/TPAMI.2020.3005434},
   issn = {19393539},
   issue = {12},
   journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
   title = {Deep Learning for 3D Point Clouds: A Survey},
   volume = {43},
   year = {2021},
}


@inproceedings{Maturana2015,
   abstract = {Robust object recognition is a crucial skill for robots operating autonomously in real world environments. Range sensors such as LiDAR and RGBD cameras are increasingly found in modern robotic systems, providing a rich source of 3D information that can aid in this task. However, many current systems do not fully utilize this information and have trouble efficiently dealing with large amounts of point cloud data. In this paper, we propose VoxNet, an architecture to tackle this problem by integrating a volumetric Occupancy Grid representation with a supervised 3D Convolutional Neural Network (3D CNN). We evaluate our approach on publicly available benchmarks using LiDAR, RGBD, and CAD data. VoxNet achieves accuracy beyond the state of the art while labeling hundreds of instances per second.},
   author = {Daniel Maturana and Sebastian Scherer},
   doi = {10.1109/IROS.2015.7353481},
   issn = {21530866},
   journal = {IEEE International Conference on Intelligent Robots and Systems},
   title = {VoxNet: A 3D Convolutional Neural Network for real-time object recognition},
   volume = {2015-December},
   year = {2015},
}

@inproceedings{Wu2015,
   abstract = {3D shape is a crucial but heavily underutilized cue in today's computer vision systems, mostly due to the lack of a good generic shape representation. With the recent availability of inexpensive 2.5D depth sensors (e.g. Microsoft Kinect), it is becoming increasingly important to have a powerful 3D shape representation in the loop. Apart from category recognition, recovering full 3D shapes from view-based 2.5D depth maps is also a critical part of visual understanding. To this end, we propose to represent a geometric 3D shape as a probability distribution of binary variables on a 3D voxel grid, using a Convolutional Deep Belief Network. Our model, 3D ShapeNets, learns the distribution of complex 3D shapes across different object categories and arbitrary poses from raw CAD data, and discovers hierarchical compositional part representation automatically. It naturally supports joint object recognition and shape completion from 2.5D depth maps, and it enables active object recognition through view planning. To train our 3D deep learning model, we construct ModelNet - a large-scale 3D CAD model dataset. Extensive experiments show that our 3D deep representation enables significant performance improvement over the-state-of-the-arts in a variety of tasks.},
   author = {Zhirong Wu and Shuran Song and Aditya Khosla and Fisher Yu and Linguang Zhang and Xiaoou Tang and Jianxiong Xiao},
   doi = {10.1109/CVPR.2015.7298801},
   issn = {10636919},
   journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
   title = {3D ShapeNets: A deep representation for volumetric shapes},
   volume = {07-12-June-2015},
   year = {2015},
}

@incollection{NEURIPS2019_9015,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {8024--8035},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}