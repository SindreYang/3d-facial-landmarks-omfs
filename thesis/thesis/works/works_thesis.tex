\documentclass[class=article, crop=false]{standalone}

\usepackage{amsfonts}
\usepackage{import}
\usepackage{multicol}
\usepackage[subpreambles=false]{standalone}

\usepackage[backend=biber]{biblatex}
\addbibresource{references.bib}

\usepackage{geometry}
\geometry{
   a4paper,
   left=20mm, right=20mm,
   top=20mm, bottom=25mm
}
 
\usepackage{hyperref}
\hypersetup{
  colorlinks=true,
  linkcolor=blue,
  citecolor=black,
}


\begin{document}




\section{Related Work}
\label{sec:works}
\subsection{Facial landmarking}
There is extensive research about 2D facial landmarking. Many non-medical tasks such as person identification, expression transfer or emotion recognition require automatic landmarking as a necessary step \cite{perrot:hal-02884592}. Existing methods for facial landmark detection can be classified into two categories: generative and discriminative. Generative methods model the facial shape as a probabilistic distribution. This category includes part-based generative models such as ASM and holistic generative models such as AAM, that capture variations in the shape or texture by Principal Component Analysis (PCA), or Gauss- Newton Deformable Part Models (GN-DPM) \cite{yan:hal-02892002}. Discriminative models take a different approach and directly look for relevant features which can be used to localize the landmarks given the input. Discriminative methods include Cascaded Regression models but also neural networks. With the emergence of Convolutional Neural Networks (CNNs), many traditional methods have been outperformed by neural networks. Most research on facial landmarking focuses on 2D. This research is focused on 3D facial landmarking with deep neural networks. As there is little research about deep learning for the case of 3D facial landmark detection, the chapter is focused on more general work on 3D deep learning.
% write about hand-crafted features

\subsection{3D Deep Learning}
In the past years, point cloud understanding is receiving increasing attention from the research community as practical applications such as autonomous driving and robotics emerge. These applications require more information than flat images can provide to obtain a better sense of the environment. The 3D data is captured by cameras with depth sensor, e.g., lidar or RGB-D cameras. 

Most authors that develop novel network architectures report their results for the more common 3D tasks classification, segmentation, object detection or shape correspondence. Since results for keypoint detection are less common, the performance results of the networks can only be regarded as a rough reference for their potential feature extraction.

Bello et al. \cite{Bello2020} identify three main challenges of deep learning with point clouds: (1) Irregularity, meaning points in the point cloud are unevenly or irregularly sampled across different regions, leaving some regions that are more sparsely, and some that are more densely occupied. (2) Unstructuredness, meaning the distance between points is not fixed, and the points can not be placed on a grid, as can be done for 2-dimensional images. (3) Unorderedness, which expresses, that a point set can be shuffled arbitrarily yielding the same point cloud, making the point set invariant to permutation.

For years, especially the unorderedness and unstructuredness of surface data hindered the performance of machine learning approaches. Early methods solve these problems by converting the point cloud into a structured grid. These methods generally fall into Voxel-based and Multi-view-based approaches.

Voxel-based approaches convert the point cloud into a volumetric occupancy grid, with the smallest unit being a fixed-size voxel. Then, 3D convolutions combined with pooling can be applied to extract features. Pioneering work that falls into this category is VoxNet \cite{Maturana2015}, which introduced a volumetric occupancy network for 3D object recognition. Wu et al. showed with ShapeNets \cite{Wu2015} that by representing 3D shapes as probability distributions of binary variables on 3D voxels and a convolutional deep belief network, encouraging performance can be achieved. However, 3D CNNs often scale poorly, as the computation and memory footprint grow cubically \cite{Guo2021}. More recent work such as the Minkovski Engine \cite{DBLP:journals/corr/abs-1904-08755} by Nvidia make use of the sparsity of voxels in space. Special operations are used for spatially sparse tensors. Conventional sparse CNNs compress networks by pruning weights to create sparse parameters but still operate on dense tensors. In contrast, the Minkovski Engine processes sparse tensors at every stage of the network including the convolutional layers.

Multi-view based methods benefit from the extensive research on 2D CNNs. Similarly to how the human eye perceives depth, a 3D image can be constructed by combining views from multiple angles. In the original work, MVCNN \cite{mvcnn}, the image is rendered from different views, a CNN is applied to each of the 2D images, and a view pooling operation combines the features. A significant drawback of this method is that the max-pooling layer only preserves the maximum value from one view. Although later methods solve this by more sophisticated view pooling, multi-view based methods still suffer from illumination differences, object occlusion and information loss during the reconstruction of the objects from different views. Furthermore, they fail to capture the underlying structure and geometry of the data.

Deep learning on point clouds has received increasing potential since Qi et al. presented PointNet \cite{Qi2017} in 2017. %By directly processing point clouds, deep learning methods have the potential to capture the intrinsic surface information better. % use this formulation for mesh?
Unlike the Euclidean case, there is no universal concept for convolutions in 3D due to the unstructuredness of the data. Different types of approaches have been developed to address this problem.
PointNet is one of the first methods to apply deep learning to unstructured and unordered inputs. PointNet learns per-point features with a Multi-Layer-Perceptron (MLP) with shared parameters. Global features are determined by symmetrical pooling functions. Symmetric functions output the same numbers independent of the input order, making them a perfect fit for dealing with unordered point sets.
Since PointNet does not consider local dependency among points, the local structure cannot be captured. Therefore, PointNet++ \cite{Qi2017b} was published that manages to improve classification and segmentation performance by modelling local regions through sampling and grouping. PointCNN \cite{Li2018} goes a step further and also takes into account the correlation between points in the local regions.

% graph based, spectral methods
An approach that operates directly on triangular mesh data is MeshCNN \cite{Hanocka2019}, which applies convolutions on edges and the four edges of their incident triangles, and pooling is applied via an edge collapse operation.
Graph-based methods generate a graph with vertices and directed edges. Feature learning can be performed either in the spatial or spectral domain. Approaches that compute the Laplacian for representing the graph operate in the spectral domain. An advantage of graph-based methods is their generality. They apply to many fields other than triangular meshes, such as for social networks, sensor networks or genetic data.

This work uses DiffusionNet \cite{sharp2022diffusion} to learn features on surfaces. DiffusionNet borrows concepts from point-based and graph-based methods by combining point-wise MLPs with a learned diffusion operation. Although the Laplacian is used to define the diffusion operator, the network itself operates in the spatial domain, as the spectral operations are merely used in the spectral expansion step to accelerate the evaluation of diffusion.

% todo: [2] Eimear Oâ€™ Sullivan and Stefanos Zafeiriou  (2020). 3D Landmark Localization in Point Clouds for the Human Ear 
% [3] Liuhao Ge, Zhou Ren, and Junsong Yuan (2018). Point-to-Point Regression PointNet for 3D Hand Pose Estimation


% How to replace convolution and pooling?
% Approaches:
% Global reductions
% Explicit geodesic convolutions
% Require costly or error-prone computations
% Too sensitive to underlying mesh structure



\end{document}
